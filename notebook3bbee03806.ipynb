{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8103533,
          "sourceType": "datasetVersion",
          "datasetId": 4785836
        },
        {
          "sourceId": 27858,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 22049,
          "modelId": 3533
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook3bbee03806",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/man-of-weirdness/MLnotebook/blob/main/notebook3bbee03806.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'vulnerable-c-source-code:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4785836%2F8103533%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240731%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240731T143147Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D65e63da850b8c4aef1d823071802dba0ae7f65d4d803e83b05b6267b501d40beae4c23aedaeda2ca2849ccd6fa7c4cd516ad3c23d44d22ebe737d552b8e9474ba12fd9a7e1d5be9a6f6ed193bd0e15635dab338ab6aa735644215e9495d0e14d4334e3f4944f2002886224f9c10545c476e7dfd369ee08c1e7eddea9a09f0a024f9466c36bd0622c0f4f994133f7c7c58f5add2e022734d8060dbf5cf92b392b20b539be930ee5c06b86ca7be6e4cbd7c531ea4d9d57ef1e137a7273b3b1edd6fbc15ccdd67986b53b803683252e42b2fa350a28909d12229590d0d96269a4d39c3934a5059817cc7e9aece8a7265ca5e806953edd23706f8f42c5b4768ab78a,gemma/keras/gemma_1.1_instruct_7b_en/3:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F22049%2F27858%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240731%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240731T143147Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D50fc14f4c13e5813de1a77dff00fd6a009ea495ebe317f1e1a635e2fefd392eb9dfea8d76a0258542ed9e64d8ce9455f51c62234924ceeb42e16c3bee6290b08411515942e7f4fad7cda5a9637513143aa1735b104a2563a8d22901fe22e5356f656d33494f861067581b1d1644b57cbfd68d6a8e04b4b9c70ad876c574d64f5dcb45cb32fd62431289148d6d3fefb62752caf2d738ad3fb937f3e99e11247401eed47da90156b4f5eb8f8be597d809538ceb16800f0360c88ab863ea357f801729864f7f2ec3519d6f6c4f5a176b1a4691b3b133e88b7c13949c56cb0ac27228aa8c9dbe1a1e10b9c2436a700f4a0103bdf2feaf8aba584e66938b57febdeb9'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "B1Fqtz1RNFeV",
        "outputId": "e19af5b9-6ee8-4fcc-9dd3-31db9003a335",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vulnerable-c-source-code, 25013129 bytes compressed\n",
            "[==================================================] 25013129 bytes downloaded\n",
            "Downloaded and uncompressed: vulnerable-c-source-code\n",
            "Downloading gemma/keras/gemma_1.1_instruct_7b_en/3, 13650669816 bytes compressed\n",
            "[=======                                           ] 2122219520 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
        "\n",
        "# Function to load files from a directory\n",
        "def load_files_from_directory(directory, label):\n",
        "    file_contents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                file_contents.append((file.read(), label))\n",
        "    return file_contents\n",
        "\n",
        "# Paths to the datasets\n",
        "non_vulnerable_dir = '/kaggle/input/vulnerable-c-source-code/Dataset_raw/Non_vulnerable'\n",
        "vulnerable_dir = '/kaggle/input/vulnerable-c-source-code/Dataset_raw/Vulnerable'\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-31T11:26:52.280614Z",
          "iopub.execute_input": "2024-07-31T11:26:52.281379Z",
          "iopub.status.idle": "2024-07-31T11:27:44.308051Z",
          "shell.execute_reply.started": "2024-07-31T11:26:52.281329Z",
          "shell.execute_reply": "2024-07-31T11:27:44.3071Z"
        },
        "trusted": true,
        "id": "Yqq48nVBNFeZ",
        "outputId": "62228ec0-0543-4ba3-d132-cb19e87e2160"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "2024-07-31 11:27:27.856024: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-31 11:27:27.856222: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-31 11:27:28.001921: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "vulnerable_files = load_files_from_directory(vulnerable_dir, 1)\n",
        "non_vulnerable_files = load_files_from_directory(non_vulnerable_dir, 0)\n",
        "\n",
        "data = vulnerable_files + non_vulnerable_files\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Create a DataFrame and split into train, validation, and test sets\n",
        "df = pd.DataFrame(data, columns=['text', 'label'])\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T11:27:44.310348Z",
          "iopub.execute_input": "2024-07-31T11:27:44.311173Z",
          "iopub.status.idle": "2024-07-31T11:28:39.414124Z",
          "shell.execute_reply.started": "2024-07-31T11:27:44.311107Z",
          "shell.execute_reply": "2024-07-31T11:28:39.413173Z"
        },
        "trusted": true,
        "id": "mawrFZf4NFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your model\n",
        "model_path = '/kaggle/input/gemma/keras/gemma_1.1_instruct_7b_en/3'\n",
        "\n",
        "# Verify if the model path contains necessary files\n",
        "if not all(os.path.isfile(os.path.join(model_path, f)) for f in ['config.json', 'model.weights.h5', 'tokenizer.json']):\n",
        "    raise FileNotFoundError(\"One or more necessary model files are missing. Ensure 'config.json', 'model.weights.h5', and 'tokenizer.json' are present in the model directory.\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T11:28:39.415196Z",
          "iopub.execute_input": "2024-07-31T11:28:39.415487Z",
          "iopub.status.idle": "2024-07-31T11:28:39.42148Z",
          "shell.execute_reply.started": "2024-07-31T11:28:39.415461Z",
          "shell.execute_reply": "2024-07-31T11:28:39.420521Z"
        },
        "trusted": true,
        "id": "rBZaxQr_NFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes==0.43.3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T14:26:57.899708Z",
          "iopub.execute_input": "2024-07-31T14:26:57.90027Z",
          "iopub.status.idle": "2024-07-31T14:28:26.26444Z",
          "shell.execute_reply.started": "2024-07-31T14:26:57.90023Z",
          "shell.execute_reply": "2024-07-31T14:28:26.262718Z"
        },
        "trusted": true,
        "id": "_x8IYGqANFea",
        "outputId": "8ec4a075-b040-48fd-9db5-600318fa1dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b7901f88460>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bitsandbytes/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b7901f88760>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bitsandbytes/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b7901f88a00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bitsandbytes/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7b7901f88bb0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/bitsandbytes/\u001b[0m\u001b[33m\n\u001b[0m^C\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model from the local directory\n",
        "import json\n",
        "import keras_nlp\n",
        "import bitsandbytes\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = keras_nlp.models.GemmaCausalLM.from_preset(model_path)\n",
        "model.backbone.enable_lora(rank=4)\n",
        "model.summary()\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,  # Adjust based on memory limits\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,  # Set to 1 for faster testing\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate(tokenized_test)\n",
        "print(results)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T12:07:14.432209Z",
          "iopub.execute_input": "2024-07-31T12:07:14.432648Z",
          "iopub.status.idle": "2024-07-31T12:07:14.482559Z",
          "shell.execute_reply.started": "2024-07-31T12:07:14.432615Z",
          "shell.execute_reply": "2024-07-31T12:07:14.4811Z"
        },
        "trusted": true,
        "id": "iiBUF_n6NFea",
        "outputId": "f4e0920f-328f-4bd0-f2b8-a461f232dae5"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras_nlp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\n\u001b[1;32m      6\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      7\u001b[0m   load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m   bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m   bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m keras_nlp\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mGemmaCausalLM\u001b[38;5;241m.\u001b[39mfrom_preset(model_path)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'bitsandbytes'",
          "output_type": "error"
        }
      ]
    }
  ]
}