{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8103533,
          "sourceType": "datasetVersion",
          "datasetId": 4785836
        },
        {
          "sourceId": 27858,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 22049,
          "modelId": 3533
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/man-of-weirdness/MLnotebook/blob/main/notebook3bbee03806.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'vulnerable-c-source-code:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4785836%2F8103533%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240801%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240801T154700Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D884f3939ec409dd2880ccc2230b8d9973beb7c77345f89e605313fffba2ce3f37d5708f8c22b3e183cd34ba78f42b55e63ca9fb24ebc8a3c6346195f5df316af420d11dd87f2b1744c0686d70c2b31c318009fd7fd3e3fcb7b4e59457cc9681ac73503a9ebbe514343d5c22e599541959415566d6fd9077682c63bf85430bf69764b04cf0b3320d366cb482aee41df4012db61230c622395a31e253e198df209364215de4f4348a44e09c1bb258068040990e8a9a1b3480c04a48c38e36e649ff8108c2f140989b8ecabe7bad21fae748756dbf196634fa54119d949e45c9fa3bd5b21bc866410ad88026b2026520b94dd506987d63646e3dca5d5b02c151536,gemma2/keras/gemma2_2b_en/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F72244%2F85984%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240801%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240801T154700Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D36443b32b463cc3c3a08d10d2b3c00937f8782e06498e5890f92d8f978a0dc23931ca433f5ee578a88c82c6e300affa0e8152e94e25b14eee128048b93d4a3c82ddba11fbc57b0a2f97d8dd09031e2a13de62eecdd5d9fcbd7645f1f251d4c992643ea73dd1ce72de508e1fee9db026cd7e8bfd79ba9ff48f2289a96806cd6eb8a52af65a36ca6b7077db56c27ec833b564097cc37b99b5292e31febd0f77472827f5a1fc464d378a6d433242a87b8e62b29256613779117348103a19ee52ea21965c1d69a8bedb7ebf12683a720c05a6f7e1cd28c77bfed4302eed25c786cdb241bbdc4887fcb95d3e9e7f27de2b3441144f689df5f818f13387ff49fda1d9d'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Fqtz1RNFeV",
        "outputId": "9c6e32b1-728d-488d-9d54-27c146840b2c"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vulnerable-c-source-code, 25013129 bytes compressed\n",
            "[==================================================] 25013129 bytes downloaded\n",
            "Downloaded and uncompressed: vulnerable-c-source-code\n",
            "Downloading gemma2/keras/gemma2_2b_en/1, 4176876349 bytes compressed\n",
            "[================================================  ] 4058234880 bytes downloaded"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "NhRaZEzDZl8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_nlp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T14:26:57.899708Z",
          "iopub.execute_input": "2024-07-31T14:26:57.90027Z",
          "iopub.status.idle": "2024-07-31T14:28:26.26444Z",
          "shell.execute_reply.started": "2024-07-31T14:26:57.90023Z",
          "shell.execute_reply": "2024-07-31T14:28:26.262718Z"
        },
        "trusted": true,
        "id": "_x8IYGqANFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow warnings\n",
        "\n",
        "# Function to load files from a directory\n",
        "def load_files_from_directory(directory, label):\n",
        "    file_contents = []\n",
        "    for filename in os.listdir(directory):\n",
        "        filepath = os.path.join(directory, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8') as file:\n",
        "                file_contents.append((file.read(), label))\n",
        "    return file_contents\n",
        "\n",
        "# Paths to the datasets\n",
        "non_vulnerable_dir = '/kaggle/input/vulnerable-c-source-code/Dataset_raw/Non_vulnerable'\n",
        "vulnerable_dir = '/kaggle/input/vulnerable-c-source-code/Dataset_raw/Vulnerable'\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-31T11:26:52.280614Z",
          "iopub.execute_input": "2024-07-31T11:26:52.281379Z",
          "iopub.status.idle": "2024-07-31T11:27:44.308051Z",
          "shell.execute_reply.started": "2024-07-31T11:26:52.281329Z",
          "shell.execute_reply": "2024-07-31T11:27:44.3071Z"
        },
        "trusted": true,
        "id": "Yqq48nVBNFeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "vulnerable_files = load_files_from_directory(vulnerable_dir, 1)\n",
        "non_vulnerable_files = load_files_from_directory(non_vulnerable_dir, 0)\n",
        "\n",
        "data = vulnerable_files + non_vulnerable_files\n",
        "np.random.shuffle(data)\n",
        "\n",
        "# Create a DataFrame and split into train, validation, and test sets\n",
        "df = pd.DataFrame(data, columns=['text', 'label'])\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T11:27:44.310348Z",
          "iopub.execute_input": "2024-07-31T11:27:44.311173Z",
          "iopub.status.idle": "2024-07-31T11:28:39.414124Z",
          "shell.execute_reply.started": "2024-07-31T11:27:44.311107Z",
          "shell.execute_reply": "2024-07-31T11:28:39.413173Z"
        },
        "trusted": true,
        "id": "mawrFZf4NFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your model\n",
        "model_path = '/kaggle/input/gemma2/keras/gemma2_2b_en/1'\n",
        "\n",
        "# Verify if the model path contains necessary files\n",
        "if not all(os.path.isfile(os.path.join(model_path, f)) for f in ['config.json', 'model.weights.h5', 'tokenizer.json']):\n",
        "    raise FileNotFoundError(\"One or more necessary model files are missing. Ensure 'config.json', 'model.weights.h5', and 'tokenizer.json' are present in the model directory.\")\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T11:28:39.415196Z",
          "iopub.execute_input": "2024-07-31T11:28:39.415487Z",
          "iopub.status.idle": "2024-07-31T11:28:39.42148Z",
          "shell.execute_reply.started": "2024-07-31T11:28:39.415461Z",
          "shell.execute_reply": "2024-07-31T11:28:39.420521Z"
        },
        "trusted": true,
        "id": "rBZaxQr_NFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model from the local directory\n",
        "import json\n",
        "import keras_nlp\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    print(gpus)\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except Exception as e:\n",
        "      print(e)\n",
        "      raise\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "try:\n",
        "    with open(os.path.join(model_path, 'tokenizer.json')) as f:\n",
        "        tokenizer_config = json.load(f)\n",
        "\n",
        "        print(\"Tokenizer config keys:\", tokenizer_config.keys())\n",
        "\n",
        "        # Assuming the custom GemmaTokenizer class is correctly imported\n",
        "        from keras_nlp.src.models.gemma import GemmaTokenizer\n",
        "\n",
        "        tokenizer = GemmaTokenizer.from_preset(model_path, load_weights=True)\n",
        "        # Now you can use the tokenizer for tokenization\n",
        "        print(\"Tokenizer loaded successfully\")\n",
        "except KeyError as e:\n",
        "    print(f\"KeyError encountered: {e}. Ensure that the tokenizer and model files are correctly formatted and located in the specified path.\")\n",
        "    raise\n",
        "\n",
        "#Loading Keras model in .h5 format\n",
        "model_file_path = os.path.join(model_path, 'model.weights.h5')\n",
        "config_file_path = os.path.join(model_path, 'config.json')\n",
        "\n",
        "# Load the model architecture\n",
        "model = keras_nlp.models.GemmaCausalLM.from_preset(model_path, load_weights=False)\n",
        "print(\"Model architecture loaded successfully\")\n",
        "model.summary()\n",
        "# Load the model weights\n",
        "if os.path.isfile(model_file_path):\n",
        "    try:\n",
        "        model.load_weights(model_file_path)\n",
        "        print(\"Model weights loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model weights: {e}\")\n",
        "else:\n",
        "    print(f\"Model weights file not found at {model_file_path}\")\n",
        "model.backbone.enable_lora(rank=4)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-31T12:07:14.432209Z",
          "iopub.execute_input": "2024-07-31T12:07:14.432648Z",
          "iopub.status.idle": "2024-07-31T12:07:14.482559Z",
          "shell.execute_reply.started": "2024-07-31T12:07:14.432615Z",
          "shell.execute_reply": "2024-07-31T12:07:14.4811Z"
        },
        "trusted": true,
        "id": "iiBUF_n6NFea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,  # Adjust based on memory limits\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,  # Set to 1 for faster testing\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "results = trainer.evaluate(tokenized_test)\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "Hv9nv4Ae_BdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}